# ğŸš€ EvalFluxX Demo Project
Ein vollstÃ¤ndiges Beispielprojekt, das zeigt, wie EvalFluxX in einer echten Anwendung eingesetzt wird.

## ğŸ“Œ Ziel des Projekts
Dieses Repository dient als **hands-on Beispiel**, um zu demonstrieren, wie EvalFluxX verwendet werden kann, um:
- eine Beispielapplikation automatisiert zu testen
- eigene Evaluations-Prompts und Metriken zu definieren
- Pipelines fÃ¼r wiederholbare Evals aufzubauen
- Ergebnisse strukturiert auszuwerten

Das Projekt ist bewusst klein und Ã¼berschaubar gehalten, sodass es ideal zum Lernen und Experimentieren ist.

---

## ğŸ§ª Beispiel-Evaluations

Das Projekt enthÃ¤lt u. a. folgende Beispiel-Evals:

### **Functional-Eval**
ÃœberprÃ¼ft, ob die App auf bestimmte Eingaben die erwarteten Ausgaben liefert.

### **Explanation-Eval**
Bewertet die QualitÃ¤t der KI-ErklÃ¤rungen oder Nutzerausgaben.

---

## ğŸ› ï¸ Einrichtung

1. Java 17 installieren und sicherstellen, dass `JAVA_HOME` gesetzt ist.
2. Maven installieren.
3. Optional: [Ollama](https://ollama.ai/) lokal mit einem Modell wie `llama3` starten (`ollama run llama3`).

## â–¶ï¸ AusfÃ¼hren

```bash
mvn clean compile
mvn exec:java -Dexec.mainClass="dev.evalfluxx.demo.App"
# fÃ¼hrt EvalFluxX Evals aus (Konfiguration via evalfluxx-plugin)
mvn evalfluxx:run
```

Die Beispiel-Implementierung nutzt LangChain4j, um Ã¼ber Ollama einen Chat-Aufruf abzusetzen. Modellname, Base-URL und Temperatur
lassen sich Ã¼ber die System-Properties `ollama.model`, `ollama.base-url`, `ollama.temperature` oder die entsprechenden
Environment-Variablen `OLLAMA_MODEL`, `OLLAMA_BASE_URL`, `OLLAMA_TEMPERATURE` anpassen.
Die AbhÃ¤ngigkeit zu EvalFluxX ist bereits in der `pom.xml` hinterlegt und kann fÃ¼r eigene Evaluations erweitert werden.
